{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ACDH Bruno Scripts",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Jigxa77ockD",
        "colab_type": "text"
      },
      "source": [
        "STEP 1: Use CrossRef API to extract the JSONs with the metadata of the Articles of DH Journals"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HuM-9r0LoYQ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Files necessary to perform the script:\n",
        "#http://doi.org/10.5281/zenodo.3406564\n",
        "#Necessary Libraries: csv, urllib, json\n",
        "\n",
        "import csv\n",
        "from csv import DictReader\n",
        "import urllib\n",
        "\n",
        "def jsondump(title, issn, filepath=\"\"):\n",
        "  query = 'http://api.crossref.org/journals/' + issn + '/works?rows=1000'\n",
        "  file = title\n",
        "  path = filepath\n",
        "  filepath = path + file.replace(\"/\",\"-\")+\".json\"\n",
        "  with open (filepath, 'w+') as jsonfile:\n",
        "    try:\n",
        "      r = urllib.request.urlopen(query)\n",
        "      data = json.loads(r.read().decode(r.info().get_param('charset') or 'utf-8'))\n",
        "      if data['message']['total-results'] > 1000: #this is necessary because the crossref API limits the maximum number of results per query @ 1000 and some journals have 1000+ publications\n",
        "        json.dump(data,jsonfile)\n",
        "        newquery = query+'&offset=1000'\n",
        "        newfile = title.replace(\"/\", \"-\")\n",
        "        newfile2 = newfile+\" part 2.json\" \n",
        "        filepath2 = path+newfile2\n",
        "        r2 = urllib.request.urlopen(newquery)\n",
        "        with open (filepath2, \"w+\") as jsonfile2:\n",
        "          data2 = json.loads(r2.read().decode(r2.info().get_param('charset') or 'utf-8'))\n",
        "          json.dump(data2,jsonfile2)\n",
        "      else:\n",
        "        json.dump(data, jsonfile)\n",
        "    except urllib.error.URLError: #this writes publication:null if the ISSN of the journal is not found using crossref api \n",
        "      data = {\"publication\":\"null\"}\n",
        "      json.dump(data, jsonfile) #alternatively you can chose not to create any file commenting this line and the line above and removing the colon besides URLError\n",
        "\n",
        "dhjournalpath = 'dh_journals.csv' #put here the path to the csv containing the DH Journals\n",
        "filepath = \"\" #put here the path to the folder where you want the results to be saved, if left blank it'll be created in the same folder as the script\n",
        "with open(dhjournalpath, 'r') as csvfile: \n",
        "  reader = DictReader(csvfile)\n",
        "  for row in reader:\n",
        "    if row['DH LEVEL'] == \"Exclusively\" or row['DH LEVEL'] == \"Significantly\":\n",
        "      a = row['TITLE']\n",
        "      b = row['E_ISSN']\n",
        "      jsondump(a,b, filepath)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9cHJLmUo_V-",
        "colab_type": "text"
      },
      "source": [
        "STEP 2: Creation of Ausiliary Dictionaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTMBICOqpDlH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#This script was used to create a dictionary with only DOI and DATES, in order\n",
        "#to speed up the process of the next script\n",
        "\n",
        "#Be aware that if you decided not to comment the lines in the script above to avoid creating files, you should manually remove the files\n",
        "#which have {publication:null} as their content\n",
        "\n",
        "import json\n",
        "import os\n",
        "\n",
        "directory = '' #insert your path here, this is the path where the json from the script above (jsondump) have been created\n",
        "doidate = dict()\n",
        "for filename in os.listdir(directory):\n",
        "    if filename.endswith(\".json\"): #be aware that the folder should only have the json files created with the previous script\n",
        "      with open(directory + filename, \"r\") as jsonfile:\n",
        "        data = json.load(jsonfile)\n",
        "        print(\"loading \" + filename)\n",
        "        for element in data[\"message\"][\"items\"]:\n",
        "          date = element[\"created\"][\"date-parts\"][0][0]\n",
        "          doi = element[\"DOI\"]\n",
        "          doidate[doi]=date\n",
        "\n",
        "tadirah_techlist = ['Information Retrieval', #these are the tadirah techniques in the DH Course Registry\n",
        " 'Encoding',\n",
        " 'Text Mining',\n",
        " 'Linked Open Data',\n",
        " 'Searching',\n",
        " 'Mapping',\n",
        " 'Georeferencing',\n",
        " 'Preservation Metadata',\n",
        " 'Scanning',\n",
        " 'Topic Modeling',\n",
        " 'Named Entity Recognition',\n",
        " 'Machine Learning',\n",
        " 'Browsing',\n",
        " 'POS-Tagging',\n",
        " 'Concordancing',\n",
        " 'Brainstorming',\n",
        " 'Pattern Recognition',\n",
        " 'Cluster Analysis',\n",
        " 'Collocation Analysis',\n",
        " 'Open Archival Information Systems',\n",
        " 'Photography',\n",
        " 'Versioning',\n",
        " 'Gamification',\n",
        " 'Web Crawling',\n",
        " 'Commenting',\n",
        " 'Distance Measures',\n",
        " 'Sentiment Analysis',\n",
        " 'Technology Preservation',\n",
        " 'Durable Persistent Media',\n",
        " 'Debugging',\n",
        " 'Principal Component Analysis',\n",
        " 'Sequence Alignment',\n",
        " 'Emulation',\n",
        " 'Replication',\n",
        " 'Bit Stream Preservation',\n",
        " 'Migration']\n",
        "\n",
        "finaldict = {}\n",
        "for tech in tadirah_techlist:  #this is to prepare the dictionary that will be populated by the next matching script\n",
        "  finaldict[tech]={2002:{\"number\":0,\"doilist\":[]},2003:{\"number\":0,\"doilist\":[]},2003:{\"number\":0,\"doilist\":[]},2004:{\"number\":0,\"doilist\":[]},2005:{\"number\":0,\"doilist\":[]},2006:{\"number\":0,\"doilist\":[]},2007:{\"number\":0,\"doilist\":[]},2008:{\"number\":0,\"doilist\":[]},2009:{\"number\":0,\"doilist\":[]},2010:{\"number\":0,\"doilist\":[]},2011:{\"number\":0,\"doilist\":[]},2012:{\"number\":0,\"doilist\":[]},2013:{\"number\":0,\"doilist\":[]},2014:{\"number\":0,\"doilist\":[]},2015:{\"number\":0,\"doilist\":[]},2016:{\"number\":0,\"doilist\":[]},2017:{\"number\":0,\"doilist\":[]},2018:{\"number\":0,\"doilist\":[]},2019:{\"number\":0,\"doilist\":[]},2020:{\"number\":0,\"doilist\":[]}}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iJX5OiurTGQ",
        "colab_type": "text"
      },
      "source": [
        "STEP 3: Matching Tadirah Techniques with Keywords of DH Publications"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1snJMw0reUZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#for this script you will need the csv generated with Microsoft Academics and both the dictionary with dois and dates and the one with dates and techniques\n",
        "import os\n",
        "import csv\n",
        "directory = '' #place the directory of the MA csvs\n",
        "for filename in os.listdir(directory):\n",
        "    if filename.endswith(\".csv\"): #be aware that you should not have csv files inside this folder other than the ones extracted from MA\n",
        "      with open(directory + filename, \"r\") as csvfile:\n",
        "        csvz = csv.DictReader(csvfile)\n",
        "        print(\"reading \" + filename)\n",
        "        for row in csvz:\n",
        "          try:\n",
        "            year = doidate[row[\"DOI\"]]\n",
        "            for item in finaldict:\n",
        "              if row[\"F.FN\"]:\n",
        "                keywords = row[\"F.FN\"].lower().replace(\"-\", \" \")\n",
        "                if item.lower().replace(\"-\",\" \") in keywords: #this is specific for POS-TAGGING  that becomes pos tagging\n",
        "                  finaldict[item][year][\"number\"]+=1\n",
        "                  finaldict[item][year][\"doilist\"].append(row[\"DOI\"])\n",
        "                  print(\"Found tadirah term \" + item + \" in article \" + row[\"DOI\"])\n",
        "          except:KeyError #the csv generated with microsoft academics might have some issues, the exception is here to prevent the code to crash on the csv errors\n",
        "\n",
        "with open(\"tadirahdhmatch.json\", \"w+\") as jsonoutput: #you can change tadirahdhmatch.json with any filename and path you want, as it stands the file will be generated in the same folder as the script\n",
        "  json.dump(finaldict, jsonoutput)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
